# -*- coding: utf-8 -*-
"""–ì–µ—Ä–∞—É—Ñ –ù.–ê. 14.3 –î–æ–º–∞—à–Ω—è—è —Ä–∞–±–æ—Ç–∞

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ie3aj2i7zHaJZ9Tsfhv15vBMImiFBg5A

# –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Ç–µ–ª–µ–≥—Ä–∞–º–º-–±–æ—Ç–∞

–í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞—Ç—å—Å—è –∏–∑ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –≤–∏–∫–∏–ø–µ–¥–∏–∏ Automotive_industry
"""

# –û—Ç–∫–ª—é—á–∏–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –≤ –∫–æ–ª–∞–±–µ. –ë—É–¥–µ—Ç –º–µ–Ω—å—à–µ –ª–∏—à–Ω–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –≤—ã–≤–æ–¥–µ
import warnings
warnings.filterwarnings('ignore')

!pip install openai mwclient mwparserfromhell tiktoken

# imports
import mwclient  # –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å MediaWiki API –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å—Ç–∞—Ç–µ–π –í–∏–∫–∏–ø–µ–¥–∏–∏
import mwparserfromhell  # –ü–∞—Ä—Å–µ—Ä –¥–ª—è MediaWiki
import openai  # –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Ç–æ–∫–∏–Ω–∏–∑–∞—Ü–∏–∏
import pandas as pd  # –í DataFrame –±—É–¥–µ–º —Ö—Ä–∞–Ω–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–∫–∏–Ω–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
import re  # –¥–ª—è –≤—ã—Ä–µ–∑–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫ <ref> –∏–∑ —Å—Ç–∞—Ç–µ–π –í–∏–∫–∏–ø–µ–¥–∏–∏
import tiktoken  # –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤

# –ó–∞–¥–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—É—é –≤–µ—Ä—Å–∏—é –í–∏–∫–∏–ø–µ–¥–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
CATEGORY_TITLE = "Category:Automotive_industry"
WIKI_SITE = "en.wikipedia.org"

# –°–æ–±–µ—Ä–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
def titles_from_category(
    category: mwclient.listing.Category, # –ó–∞–¥–∞–µ–º —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å—Ç–∞—Ç–µ–π
    max_depth: int # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–ª—É–±–∏–Ω—É –≤–ª–æ–∂–µ–Ω–∏—è —Å—Ç–∞—Ç–µ–π
) -> set[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–±–æ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –¥–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –í–∏–∫–∏–ø–µ–¥–∏–∏ –∏ –µ–µ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö."""
    titles = set() # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å—Ç–∞—Ç–µ–π
    for cm in category.members(): # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if type(cm) == mwclient.page.Page: # –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç —è–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
            titles.add(cm.name) # –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º –∏–º—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0: # –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç —è–≤–ª—è–µ—Ç—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π –∏ –≥–ª—É–±–∏–Ω–∞ –≤–ª–æ–∂–µ–Ω–∏—è –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π
            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1) # –≤—ã–∑—ã–≤–∞–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            titles.update(deeper_titles) # –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–∑ –¥—Ä—É–≥–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞
    return titles

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–∞ MediaWiki
# WIKI_SITE —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—É—é —á–∞—Å—Ç—å –í–∏–∫–∏–ø–µ–¥–∏–∏
site = mwclient.Site(WIKI_SITE)

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–∑–¥–µ–ª–∞ –∑–∞–¥–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
category_page = site.pages[CATEGORY_TITLE]
# –ü–æ–ª—É—á–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –≤—Å–µ—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å—é –Ω–∞ –æ–¥–∏–Ω —É—Ä–æ–≤–µ–Ω—å
titles = titles_from_category(category_page, max_depth=1)


print(f"–°–æ–∑–¥–∞–Ω–æ {len(titles)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å—Ç–∞—Ç–µ–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {CATEGORY_TITLE}.")

# –ó–∞–¥–∞–µ–º —Å–µ–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –æ—Ç–±—Ä–æ—à–µ–Ω—ã –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç–µ–π
SECTIONS_TO_IGNORE = [
    "See also",
    "References",
    "External links",
    "Further reading",
    "Footnotes",
    "Bibliography",
    "Sources",
    "Citations",
    "Literature",
    "Footnotes",
    "Notes and references",
    "Photo gallery",
    "Works cited",
    "Photos",
    "Gallery",
    "Notes",
    "References and sources",
    "References and notes",
]

# –§—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–µ–∫—Ü–∏–π –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –í–∏–∫–∏–ø–µ–¥–∏–∏

def all_subsections_from_section(
    section: mwparserfromhell.wikicode.Wikicode, # —Ç–µ–∫—É—â–∞—è —Å–µ–∫—Ü–∏—è
    parent_titles: list[str], # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Ä–æ–¥–∏—Ç–µ–ª—è
    sections_to_ignore: set[str], # –°–µ–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å
) -> list[tuple[list[str], str]]:
    """
    –ò–∑ —Ä–∞–∑–¥–µ–ª–∞ –í–∏–∫–∏–ø–µ–¥–∏–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–µ–∫—Ü–∏–π.
    –ö–∞–∂–¥—ã–π –ø–æ–¥—Ä–∞–∑–¥–µ–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ—Ä—Ç–µ–∂, –≥–¥–µ:
      - –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–ø–∏—Å–æ–∫ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —Å–µ–∫—Ü–∏–π, –Ω–∞—á–∏–Ω–∞—è —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
      - –≤—Ç–æ—Ä–æ–π —ç–ª–µ–º–µ–Ω—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ç–µ–∫—Å—Ç —Å–µ–∫—Ü–∏–∏
    """

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–µ–∫—É—â–µ–π —Å–µ–∫—Ü–∏–∏
    headings = [str(h) for h in section.filter_headings()]
    title = headings[0]
    # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –í–∏–∫–∏–ø–µ–¥–∏–∏ –∏–º–µ—é—Ç –≤–∏–¥: "== Heading =="

    if title.strip("=" + " ") in sections_to_ignore:
        # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏ –≤ —Å–ø–∏—Å–∫–µ –¥–ª—è –∏–≥–Ω–æ—Ä–∞, —Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ
        return []

    # –û–±—ä–µ–¥–∏–Ω–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∏, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è chatGPT
    titles = parent_titles + [title]

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º wikicode —Å–µ–∫—Ü–∏–∏ –≤ —Å—Ç—Ä–æ–∫—É
    full_text = str(section)

    # –í—ã–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å–µ–∫—Ü–∏–∏ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    section_text = full_text.split(title)[1]
    if len(headings) == 1:
        # –ï—Å–ª–∏ –æ–¥–∏–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫, —Ç–æ —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π —Å–ø–∏—Å–æ–∫
        return [(titles, section_text)]
    else:
        first_subtitle = headings[1]
        section_text = section_text.split(first_subtitle)[0]
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π —Å–ø–∏—Å–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ –ø–µ—Ä–≤–æ–≥–æ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞
        results = [(titles, section_text)]
        for subsection in section.get_sections(levels=[len(titles) + 1]):
            results.extend(
                # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–ª—É—á–µ–Ω–∏—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–µ–∫—Ü–∏–π –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏
                all_subsections_from_section(subsection, titles, sections_to_ignore)
                )  # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ —Å–ø–∏—Å–∫–∏ –¥–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –≤—ã–∑—ã–≤–∞–µ–º–æ–π
        return results

# –§—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–µ–∫—Ü–∏–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º —Ç–µ—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º
def all_subsections_from_title(
    title: str, # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏ –í–∏–∫–∏–ø–µ–¥–∏–∏, –∫–æ—Ç–æ—Ä—É—é –ø–∞—Ä—Å–∏–º
    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE, # –°–µ–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
    site_name: str = WIKI_SITE, # –°—Å—ã–ª–∫–∞ –Ω–∞ —Å–∞–π—Ç –≤–∏–∫–∏–ø–µ–¥–∏–∏
) -> list[tuple[list[str], str]]:
    """
    –ò–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –í–∏–∫–∏–ø–µ–¥–∏–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–µ–∫—Ü–∏–π.
    –ö–∞–∂–¥—ã–π –ø–æ–¥—Ä–∞–∑–¥–µ–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ—Ä—Ç–µ–∂, –≥–¥–µ:
      - –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–ø–∏—Å–æ–∫ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —Å–µ–∫—Ü–∏–π, –Ω–∞—á–∏–Ω–∞—è —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
      - –≤—Ç–æ—Ä–æ–π —ç–ª–µ–º–µ–Ω—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ç–µ–∫—Å—Ç —Å–µ–∫—Ü–∏–∏
    """

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–∞ MediaWiki
    # WIKI_SITE —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—É—é —á–∞—Å—Ç—å –í–∏–∫–∏–ø–µ–¥–∏–∏
    site = mwclient.Site(site_name)

    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
    page = site.pages[title]

    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    text = page.text()

    # –£–¥–æ–±–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è MediaWiki
    parsed_text = mwparserfromhell.parse(text)
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
    headings = [str(h) for h in parsed_text.filter_headings()]
    if headings: # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞–π–¥–µ–Ω—ã
        # –í –∫–∞—á–µ—Å—Ç–≤–µ —Ä–µ–∑—é–º–µ –±–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        summary_text = str(parsed_text).split(headings[0])[0]
    else:
        # –ï—Å–ª–∏ –Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, —Ç–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—á–∏—Ç–∞–µ–º —Ä–µ–∑—é–º–µ
        summary_text = str(parsed_text)
    results = [([title], summary_text)] # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—é–º–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π —Å–ø–∏—Å–æ–∫
    for subsection in parsed_text.get_sections(levels=[2]): # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–∫—Ü–∏–∏ 2-–≥–æ —É—Ä–æ–≤–Ω—è
        results.extend(
            # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–ª—É—á–µ–Ω–∏—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–µ–∫—Ü–∏–π –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏
            all_subsections_from_section(subsection, [title], sections_to_ignore)
        ) # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ —Å–ø–∏—Å–∫–∏ –¥–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –≤—ã–∑—ã–≤–∞–µ–º–æ–π
    return results

# –†–∞–∑–±–∏–≤–∫–∞ —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å–µ–∫—Ü–∏–∏
# –ø—Ä–∏–¥–µ—Ç—Å—è –Ω–µ–º–Ω–æ–≥–æ –ø–æ–¥–æ–∂–¥–∞—Ç—å, —Ç–∞–∫ –∫–∞–∫ –Ω–∞ –ø–∞—Ä—Å–∏–Ω–≥ 100 —Å—Ç–∞—Ç–µ–π —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–∫–æ–ª–æ –º–∏–Ω—É—Ç—ã
wikipedia_sections = []
for title in titles:
    wikipedia_sections.extend(all_subsections_from_title(title))
print(f"–ù–∞–π–¥–µ–Ω–æ {len(wikipedia_sections)} —Å–µ–∫—Ü–∏–π –Ω–∞ {len(titles)} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö")

# –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å–µ–∫—Ü–∏–∏ –æ—Ç —Å—Å—ã–ª–æ–∫ <ref>xyz</ref>, –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∏ –∫–æ–Ω–µ—á–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:
    titles, text = section
    # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏
    text = re.sub(r"<ref.*?</ref>", "", text)
    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
    text = text.strip()
    return (titles, text)

# –ü—Ä–∏–º–µ–Ω–∏–º —Ñ—É–Ω–∫—Ü–∏—é –æ—á–∏—Å—Ç–∫–∏ –∫–æ –≤—Å–µ–º —Å–µ–∫—Ü–∏—è–º —Å –ø–æ–º–æ—â—å—é –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–ø–∏—Å–∫–æ–≤
wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]

# –û—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –ø—É—Å—Ç—ã–µ —Å–µ–∫—Ü–∏–∏
def keep_section(section: tuple[list[str], str]) -> bool:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ True, –µ—Å–ª–∏ —Ä–∞–∑–¥–µ–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω, –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –∑–Ω–∞—á–µ–Ω–∏–µ False."""
    titles, text = section
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π –¥–ª–∏–Ω–µ, –º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –∏ –¥—Ä—É–≥–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    if len(text) < 16:
        return False
    else:
        return True


original_num_sections = len(wikipedia_sections)
wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]
print(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {original_num_sections-len(wikipedia_sections)} —Å–µ–∫—Ü–∏–π, –æ—Å—Ç–∞–ª–æ—Å—å {len(wikipedia_sections)} —Å–µ–∫—Ü–∏–π.")

for ws in wikipedia_sections[:5]:
    print(ws[0])
    display(ws[1][:50] + "...")
    print()

GPT_MODEL = "gpt-3.5-turbo"  # only matters insofar as it selects which tokenizer to use

# –§—É–Ω–∫—Ü–∏—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
def num_tokens(text: str, model: str = GPT_MODEL) -> int:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ."""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# –§—É–Ω–∫—Ü–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä–æ–∫
def halved_by_delimiter(string: str, delimiter: str = "\n") -> list[str, str]:
    """–†–∞–∑–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–æ–∫—É –Ω–∞–¥–≤–æ–µ —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è (delimiter), –ø—ã—Ç–∞—è—Å—å —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã —Å –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã."""

    # –î–µ–ª–∏–º —Å—Ç—Ä–æ–∫—É –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—é, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é \n - –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
    chunks = string.split(delimiter)
    if len(chunks) == 1:
        return [string, ""]  # —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω
    elif len(chunks) == 2:
        return chunks  # –Ω–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏—Å–∫–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é —Ç–æ—á–∫—É
    else:
        # –°—á–∏—Ç–∞–µ–º —Ç–æ–∫–µ–Ω—ã
        total_tokens = num_tokens(string)
        halfway = total_tokens // 2
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ—Ä–µ–¥–∏–Ω–µ —á–∏—Å–ª–∞ —Ç–æ–∫–µ–Ω–æ–≤
        best_diff = halfway
        # –í —Ü–∏–∫–ª–µ –∏—â–µ–º –∫–∞–∫–æ–π –∏–∑ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π, –±—É–¥–µ—Ç –±–ª–∏–∂–µ –≤—Å–µ–≥–æ –∫ best_diff
        for i, chunk in enumerate(chunks):
            left = delimiter.join(chunks[: i + 1])
            left_tokens = num_tokens(left)
            diff = abs(halfway - left_tokens)
            if diff >= best_diff:
                break
            else:
                best_diff = diff
        left = delimiter.join(chunks[:i])
        right = delimiter.join(chunks[i:])
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª–µ–≤—É—é –∏ –ø—Ä–∞–≤—É—é —á–∞—Å—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        return [left, right]


# –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–µ–∑–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞ —Ç–æ–∫–µ–Ω–æ–≤
def truncated_string(
    string: str, # —Å—Ç—Ä–æ–∫–∞
    model: str, # –º–æ–¥–µ–ª—å
    max_tokens: int, # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    print_warning: bool = True, # —Ñ–ª–∞–≥ –≤—ã–≤–æ–¥–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
) -> str:
    """–û–±—Ä–µ–∑–∫–∞ —Å—Ç—Ä–æ–∫–∏ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞ —Ç–æ–∫–µ–Ω–æ–≤."""
    encoding = tiktoken.encoding_for_model(model)
    encoded_string = encoding.encode(string)
    # –û–±—Ä–µ–∑–∞–µ–º —Å—Ç—Ä–æ–∫—É –∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ
    truncated_string = encoding.decode(encoded_string[:max_tokens])
    if print_warning and len(encoded_string) > max_tokens:
        print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –°—Ç—Ä–æ–∫–∞ –æ–±—Ä–µ–∑–∞–Ω–∞ —Å {len(encoded_string)} —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤.")
    # –£—Å–µ—á–µ–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
    return truncated_string

# –§—É–Ω–∫—Ü–∏—è –¥–µ–ª–∏—Ç —Å–µ–∫—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º—É —á–∏—Å–ª—É —Ç–æ–∫–µ–Ω–æ–≤
def split_strings_from_subsection(
    subsection: tuple[list[str], str], # —Å–µ–∫—Ü–∏–∏
    max_tokens: int = 1000, # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤
    model: str = GPT_MODEL, # –º–æ–¥–µ–ª—å
    max_recursion: int = 5, # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ä–µ–∫—É—Ä—Å–∏–π
) -> list[str]:
    """
    –†–∞–∑–¥–µ–ª—è–µ—Ç —Å–µ–∫—Ü–∏–∏ –Ω–∞ —Å–ø–∏—Å–æ–∫ –∏–∑ —á–∞—Å—Ç–µ–π —Å–µ–∫—Ü–∏–π, –≤ –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏ –Ω–µ –±–æ–ª–µ–µ max_tokens.
    –ö–∞–∂–¥–∞—è —á–∞—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ—Ä—Ç–µ–∂ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ [H1, H2, ...] –∏ —Ç–µ–∫—Å—Ç–∞ (str).
    """
    titles, text = subsection
    string = "\n\n".join(titles + [text])
    num_tokens_in_string = num_tokens(string)
    # –ï—Å–ª–∏ –¥–ª–∏–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –¥–æ–ø—É—Å—Ç–∏–º–æ–π, —Ç–æ –≤–µ—Ä–Ω–µ—Ç —Å—Ç—Ä–æ–∫—É
    if num_tokens_in_string <= max_tokens:
        return [string]
    # –µ—Å–ª–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —Ä–µ–∫—É—Ä—Å–∏—è –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É, —Ç–æ –ø—Ä–æ—Å—Ç–æ —É—Å–µ—á–µ–º –µ–µ –ø–æ —á–∏—Å–ª—É —Ç–æ–∫–µ–Ω–æ–≤
    elif max_recursion == 0:
        return [truncated_string(string, model=model, max_tokens=max_tokens)]
    # –∏–Ω–∞—á–µ —Ä–∞–∑–¥–µ–ª–∏–º –ø–æ–ø–æ–ª–∞–º –∏ –≤—ã–ø–æ–ª–Ω–∏–º —Ä–µ–∫—É—Ä—Å–∏—é
    else:
        titles, text = subsection
        for delimiter in ["\n\n", "\n", ". "]: # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É (—Ä–∞–∑—Ä—ã–≤, –∞–±–∑–∞—Ü, —Ç–æ—á–∫–∞)
            left, right = halved_by_delimiter(text, delimiter=delimiter)
            if left == "" or right == "":
                # –µ—Å–ª–∏ –∫–∞–∫–∞—è-–ª–∏–±–æ –ø–æ–ª–æ–≤–∏–Ω–∞ –ø—É—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É —Å –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º
                continue
            else:
                # –ø—Ä–∏–º–µ–Ω–∏–º —Ä–µ–∫—É—Ä—Å–∏—é –Ω–∞ –∫–∞–∂–¥–æ–π –ø–æ–ª–æ–≤–∏–Ω–µ
                results = []
                for half in [left, right]:
                    half_subsection = (titles, half)
                    half_strings = split_strings_from_subsection(
                        half_subsection,
                        max_tokens=max_tokens,
                        model=model,
                        max_recursion=max_recursion - 1, # —É–º–µ–Ω—å—à–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ä–µ–∫—É—Ä—Å–∏–π
                    )
                    results.extend(half_strings)
                return results
    # –∏–Ω–∞—á–µ –Ω–∏–∫–∞–∫–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–æ –Ω–µ –±—ã–ª–æ, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ—Å—Ç–æ –æ–±—Ä–µ–∑–∞–µ–º —Å—Ç—Ä–æ–∫—É (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—á–µ–Ω—å —Ä–µ–¥–∫–æ)
    return [truncated_string(string, model=model, max_tokens=max_tokens)]

# –î–µ–ª–∏–º —Å–µ–∫—Ü–∏–∏ –Ω–∞ —á–∞—Å—Ç–∏
MAX_TOKENS = 1600
wikipedia_strings = []
for section in wikipedia_sections:
    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))

print(f"{len(wikipedia_sections)} —Å–µ–∫—Ü–∏–π –í–∏–∫–∏–ø–µ–¥–∏–∏ –ø–æ–¥–µ–ª–µ–Ω—ã –Ω–∞ {len(wikipedia_strings)} —Å—Ç—Ä–æ–∫.")

# –ù–∞–ø–µ—á–∞—Ç–∞–µ–º –ø—Ä–∏–º–µ—Ä —Å—Ç—Ä–æ–∫–∏
print(wikipedia_strings[1])

from openai import OpenAI
import os
import getpass

EMBEDDING_MODEL = "text-embedding-ada-002"  # –ú–æ–¥–µ–ª—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –æ—Ç OpenAI

os.environ["OPENAI_API_KEY"] = getpass.getpass("–í–≤–µ–¥–∏—Ç–µ OpenAI API Key:")
client = OpenAI(api_key = os.environ.get("OPENAI_API_KEY"))

# –§—É–Ω–∫—Ü–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏ chatGPT —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –µ–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤)
def get_embedding(text, model="text-embedding-ada-002"):

   return client.embeddings.create(input = [text], model=model).data[0].embedding

df = pd.DataFrame({"text": wikipedia_strings[:10]})

df['embedding'] = df.text.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))

SAVE_PATH = "./Automotive_industry"

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
df.to_csv(SAVE_PATH, index=False)

df.head()

"""# –≠—Ç–∞–ø 2: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–ª–µ–≥—Ä–∞–º–º –±–æ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–≥–æ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π



---

* –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
"""

!pip install openai
!pip install --upgrade aiogram --pre
!pip install nest_asyncio

"""* –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞"""

import nest_asyncio
nest_asyncio.apply()

import asyncio
from aiogram import Bot, Dispatcher, Router, types
from aiogram.filters import Command
import getpass
import pandas as pd
import numpy as np
import openai
import ast

"""* –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""

SAVE_PATH = "./Automotive_industry"
df = pd.read_csv(SAVE_PATH)

"""* –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""

async def get_embedding(text, model="text-embedding-ada-002"):
    text = text.replace("\n", " ")
    response = await openai.Embedding.acreate(
        input=text,
        model=model
    )
    embedding = response['data'][0]['embedding']
    return embedding

if 'embedding' not in df.columns or df['embedding'].isnull().any():
    print("–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã. –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")

    async def compute_embeddings():
        embeddings = []
        for text in df['text']:
            embedding = await get_embedding(text)
            embeddings.append(embedding)
        df['embedding'] = embeddings
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å –Ω–æ–≤—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
        df.to_csv(SAVE_PATH, index=False)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
    await compute_embeddings()
else:
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫–∏ –≤ —Å–ø–∏—Å–∫–∏ —á–∏—Å–µ–ª
    df['embedding'] = df['embedding'].apply(ast.literal_eval)

"""* –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞"""

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ OpenAI API –∫–ª—é—á–∞
openai.api_key = getpass.getpass("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à OpenAI API Key:")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞
bot_token = getpass.getpass("–í–≤–µ–¥–∏—Ç–µ —Ç–æ–∫–µ–Ω –≤–∞—à–µ–≥–æ –±–æ—Ç–∞:")
bot = Bot(token=bot_token)
dp = Dispatcher()
router = Router()

"""* –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥ /start –∏ /help"""

@router.message(Command("start"))
async def cmd_start(message: types.Message):
    welcome_text = (
        "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–µ–º–µ '–ê–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–∞—è –∏–Ω–¥—É—Å—Ç—Ä–∏—è'.\n"
        "–í–≤–µ–¥–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É /help –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
    )
    await message.answer(welcome_text)

@router.message(Command("help"))
async def cmd_help(message: types.Message):
    knowledge_base_info = (
        "üìö *–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π:*\n"
        f"- –¢–µ–º–∞—Ç–∏–∫–∞: *Automotive Industry*\n"
        f"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: *{len(df)}*\n\n"
        "üí° *–ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞:* 'What is the history of the automotive industry?'\n\n"
        "–ó–∞–¥–∞–π—Ç–µ –º–Ω–µ –≤–æ–ø—Ä–æ—Å –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ, –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –ø–æ–º–æ—á—å!"
    )
    await message.answer(knowledge_base_info, parse_mode="Markdown")

"""* –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤"""

def search_knowledge_base(user_embedding, df, n=5):
    embeddings = np.array(df['embedding'].tolist())
    user_embedding = np.array(user_embedding)
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    user_embedding_norm = user_embedding / np.linalg.norm(user_embedding)
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
    similarities = np.dot(embeddings_norm, user_embedding_norm)
    df['similarity'] = similarities
    top_n = df.nlargest(n, 'similarity')
    return top_n

"""* –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""

async def generate_answer(question, context):
    prompt = (
        f"–í–æ–ø—Ä–æ—Å: {question}\n\n"
        f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n"
        "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç."
    )
    response = await openai.ChatCompletion.acreate(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=500,
        n=1,
    )
    answer = response['choices'][0]['message']['content'].strip()
    return answer

"""* –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""

@router.message()
async def handle_user_message(message: types.Message):
    user_question = message.text

    user_embedding = await get_embedding(user_question)

    top_n = search_knowledge_base(user_embedding, df, n=5)

    context = "\n\n".join(top_n['text'].tolist())

    max_context_length = 3000
    context = context[:max_context_length]

    answer = await generate_answer(user_question, context)

    await message.answer(answer)

"""* –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ä–æ—É—Ç–µ—Ä–∞"""

dp.include_router(router)

"""* –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""

async def main():
    await dp.start_polling(bot)

await main()